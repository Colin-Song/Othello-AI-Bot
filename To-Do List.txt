How Our Algo Runs
	1. Choose root node
	2. Apply eval function to children of root node and select best children
	3. Apply MCTS on child where opp's move uses heuristic function in simulation
	4. Have minimum of x number of games for each child node (don't want to miss out on a good move just because first x-n simulations were losses)
	5. Store simulation games
	6. If child node leads to many losses, move on to next child node and repeat steps 3-5
	7. If child node has many wins and continues to simulate until time expires, make this move and delete simulations that did not have this move
	8. Opp makes move
	9. Get current state and from saved simulations, delete all states that do not have latest opp's move

Things We Need To Do:
	- implement best moves into MCTS
	- implement depth limit
	- in MCTS select opp's move using heuristic function

Heuristic Function
	Positional value (value board)
	Mobility (number of our valid moves - number of opp valid moves)
	Stability (number of our stable discs - number of opp stable discs)
	Frontier discs (number of opp frontiers - number of our frontiers)
	Greed Penalty?

	Function = w1*PositionalValue + w2*Mobility + w3*Stability + w4*Frontier

	Get valid moves list
	Iterate through valid moves list
		Apply function on move
			(For move get positional value of that move,
			Get mobility of board after the move,
			Get stability of board after the move,
			Get frontier discs after the move)
		Store this value/move